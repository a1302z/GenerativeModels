# Playground project for generative models in PyTorch
In this project I want to implement and try several approaches on generating artificial data from scratch. 
# About data
As these are all Machine Learning models they learn from given data. We are using two different datasets until now. [MNIST](http://yann.lecun.com/exdb/mnist/) and [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html). However, it is straightforward to extend implementation to other datasets.
# 1. Autoencoders

<details>
 <summary><b>Architecture details</b></summary><br/>
  
The models are all based on two basic building blocks.
  
  1. <b>Down Block</b> <br/>
    - This block is used in the encoder. It divides the resolution by two while doubling the number of channels.
  2. <b>Up Block </b> <br/>
    - The inversion of the Down Block for the decoder. Doubles resolution by halving the number of channels.
 </details>
 
<details>
 <summary><b>Plain Autoencoders</b></summary><br/>
  
Autoencoders aim to reconstruct their input with a bottleneck, hence having to learn how to best encode data with least loss of information. <br/>
We have two models for classical autoencoders.
<details>
 <summary><b>Fully Convolutional model</b></summary><br/>
  
This model decreases the image resolution while enlarging the number of channels. However, it is fully convolutional, without any fully connected layers. In the bottleneck data therefore is represented by a small resolution image with a large number of channels. <br/>
<b> Reconstruction results for MNIST and CelebA. (Left: original/Right: reconstructed) </b> <br/>

| ![Autoencoder MNIST](result_figures/AE_MNIST_reconstruction.png) | ![Autoencoder CelebA](result_figures/AE_CelebA_reconstruction.png) |
|:--:|:--:|
| MNIST | CelebA |

</details>

<details>
 <summary><b>Convolutional model with linear hidden dimension</b></summary><br/>
  
The model is based on the Fully Convolutional model. However, it enforces a stronger encoding by having a n-dimensional hidden vector as bottleneck where n is small (e.g. 16). This is done by having a Fully Connected Layer from the convolutional representation and one more back. <br/>
<b> Reconstructions results for Autoencoder with hidden vector </b> <br/>

| ![Linear autoencoder result](result_figures/AE_linear_MNIST_reconstruction.png) | ![Linear autoencoder result](result_figures/AE_linear_CelebA_reconstruction.png) |
|:--:|:--:|
|MNIST|CelebA|

<!--- Encoding visualized in hidden space \
![Linear autoencoder hidden space](result_figures/2d_encoding_AE_linear.png) --->
#### t-SNE representation of hidden space
To see how well the model is seperating classes, we sample from the test set and visualize their hidden represention using t-SNE. We use MNIST as we have class labels. <br/>
![TSNE of hidden representation for linear autoencoder on MNIST](result_figures/tsne_AE_linear_MNIST.png)
 </details>
 </details>

<details>
 <summary><b> Variational Autoencoders (VAE) </b></summary><br/>
  
Variational autoencoders similarily to Autoencoders trying to find a good hidden encoding for reconstruction of input data. However, they encode to a mean and variance, where the minimization of KL-divergence to a standard normal distribution is part of optimization objective. Thus artificial data can be generated by sampling from a standard normal distribution and decode these. <br/>
<b> Reconstruction of given test samples </b> <br/>

| ![Reconstructed samples VAE MNIST](result_figures/VAE_MNIST_reconstruction.png) | ![Reconstructed samples VAE CelebA](result_figures/VAE_CelebA_reconstruction.png) |
|:--:|:--:|
|MNIST|CelebA|

<b> Randomly generated artificial samples </b> <br/>

| ![Artificial MNIST](result_figures/artificial_samples_MNIST.png) | ![Artificial CelebA](result_figures/artificial_samples_CelebA.png) |
|:--:|:--:|

<b> Reconstructed samples from linear grid in two dimensions </b> <br/>
![VAE generated samples MNIST](result_figures/VAE_generation.png) <br/>
</details>
</details>

<!--- Visualization of encoding of mean values in hidden 2d space. Note how it is much more centered around zero. \
![VAE encoding](result_figures/2d_encoding_VAE.png) --->

# 2. Generative Adverserial Networks (GANs)
<details>
 <summary><b> Vanilla GAN </b></summary><br/>
  
  Original GANs (Goodfellow et al) that are based upon linear layers in generator and discriminator.
  ![MNIST_Vanilla_result](result_figures/VanillaGAN_MNIST_2020-01-14_18:49.png)
  
</details>

<details>
 <summary><b> DC GAN </b></summary><br/>
  
  Concept of GANs that utilize convolutional layers in generator and discriminator.
  ![MNIST_DCGAN_result](result_figures/DCGAN_MNIST_2020-01-14_20:50.png)
  
</details>

<details>
 <summary><b> Auxillary GAN </b></summary><br/>
  
  DC GAN but labels of dataset are used as generator input and discriminator output.
  ![MNIST_AUX_DCGAN_result](result_figures/DCGAN_MNIST_2020-01-16_12:36.png)
</details>
